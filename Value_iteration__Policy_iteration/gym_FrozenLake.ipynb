{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'FrozenLake-v0' gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy Iteration](https://miro.medium.com/max/2400/1*07AboseYfdjqknCq0kG8Ow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Policy Iteration (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏) –¥–ª—è –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è $\\pi \\approx \\pi_*$\n",
    "\n",
    "#### 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "–ó–∞–¥–∞—Ç—å $V(s) \\in \\mathbb{R}$ –∏ $\\pi(s) \\in \\mathcal{A}(s)$ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ –¥–ª—è –≤—Å–µ—Ö $s \\in \\mathcal{S}$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (Policy Evaluation)\n",
    "\n",
    "–¶–∏–∫–ª:\n",
    "\n",
    "* $\\Delta \\leftarrow 0$\n",
    "* –¶–∏–∫–ª –ø–æ –∫–∞–∂–¥–æ–º—É $s \\in \\mathcal{S}$:\n",
    "\n",
    "  * $v \\leftarrow V(s)$\n",
    "  * $V(s) \\leftarrow \\sum_{s', r} p(s', r \\mid s, \\pi(s)) \\left[ r + \\gamma V(s') \\right]$\n",
    "  * $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n",
    "\n",
    "–ü–æ–∫–∞ $\\Delta \\geq \\theta$\n",
    "(–≥–¥–µ $\\theta$ ‚Äî –Ω–µ–±–æ–ª—å—à–æ–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ, –∑–∞–¥–∞—é—â–µ–µ —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. –£–ª—É—á—à–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (Policy Improvement)\n",
    "\n",
    "* $\\text{policy-stable} \\leftarrow \\text{true}$\n",
    "* –î–ª—è –∫–∞–∂–¥–æ–≥–æ $s \\in \\mathcal{S}$:\n",
    "\n",
    "  * $\\text{old-action} \\leftarrow \\pi(s)$\n",
    "  * $\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s',r} p(s', r \\mid s,a)\\left[ r + \\gamma V(s') \\right]$\n",
    "  * –ï—Å–ª–∏ $\\text{old-action} \\neq \\pi(s)$, —Ç–æ $\\text{policy-stable} \\leftarrow \\text{false}$\n",
    "\n",
    "–ï—Å–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–∞ ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –∏ –≤–µ—Ä–Ω—É—Ç—å $V \\approx v_*$, $\\pi \\approx \\pi_*$;\n",
    "–∏–Ω–∞—á–µ –ø–µ—Ä–µ–π—Ç–∏ –∫ —à–∞–≥—É 2.\n",
    "\n",
    "---\n",
    "\n",
    "–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –º–æ–≥—É –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å —ç—Ç–æ –∫–∞–∫ Python-—Ñ—É–Ω–∫—Ü–∏—é, –ø–æ–Ω—è—Ç–Ω—É—é –¥–ª—è Jupyter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–ø–∏—Å–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ policy iteration\n",
    "\n",
    "**policy**: –î–≤—É–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤ —Ä–∞–∑–º–µ—Ä–∞ n(S) √ó n(A), –∫–∞–∂–¥–∞—è —è—á–µ–π–∫–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è a –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ s.\n",
    "\n",
    "**environment**: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç —Å—Ä–µ–¥—ã OpenAI Gym\n",
    "\n",
    "**discount_factor**: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è MDP\n",
    "\n",
    "**theta**: –ü–æ—Ä–æ–≥ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏. –ö–æ–≥–¥–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –º–µ–Ω—å—à–µ —ç—Ç–æ–≥–æ —á–∏—Å–ª–∞, –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–∫—Ä–∞—â–∞—é—Ç—Å—è.\n",
    "\n",
    "**max_iterations**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—ë—Ç –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ nS, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.\n",
    "\n",
    "---\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –Ω–∞—á–Ω—ë–º —Å —à–∞–≥–∞ –æ—Ü–µ–Ω–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (policy evaluation). –¶–µ–ª—å ‚Äî —Å—Ö–æ–¥–∏—Ç—å—Å—è –∫ –∏—Å—Ç–∏–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ œÄ. –ú—ã –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –Ω—É–∂–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ—Ü–µ–Ω–∫–∏\n",
    "    evaluation_iterations = 1\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω—É–ª—ë–º\n",
    "    V = np.zeros(environment.observation_space.n)\n",
    "    # –ü–æ–≤—Ç–æ—Ä—è–µ–º, –ø–æ–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –º–µ–Ω—å—à–µ –ø–æ—Ä–æ–≥–∞\n",
    "    for i in range(int(max_iterations)):\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –Ω—É–ª—ë–º\n",
    "        delta = 0\n",
    "        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é\n",
    "        for state in range(environment.observation_space.n):\n",
    "           # –ù–∞—á–∞–ª—å–Ω–æ–µ –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "           v = 0\n",
    "           # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ —ç—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "           for action, action_probability in enumerate(policy[state]):\n",
    "             # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–º –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ\n",
    "             for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                  # –í—ã—á–∏—Å–ª—è–µ–º –æ–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "                  v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
    "\n",
    "           # –í—ã—á–∏—Å–ª—è–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "           delta = max(delta, np.abs(V[state] - v))\n",
    "           # –û–±–Ω–æ–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "           V[state] = v\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        # –ó–∞–≤–µ—Ä—à–∞–µ–º, –µ—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ\n",
    "        if delta < theta:\n",
    "            print(f'–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ü–µ–Ω–µ–Ω–∞ –∑–∞ {evaluation_iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π.')\n",
    "            return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —á–∞—Å—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (policy iteration).  \n",
    "–ù–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –¥–µ–ª–∞—Ç—å ¬´–æ–¥–∏–Ω —à–∞–≥ –≤–ø–µ—Ä—ë–¥¬ª (one-step lookahead), —á—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è.  \n",
    "–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –º–∞—Å—Å–∏–≤ –¥–ª–∏–Ω—ã nA, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –æ–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "    action_values = np.zeros(environment.action_space.n)\n",
    "    for action in range(environment.action_space.n):\n",
    "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –æ–±—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (policy iteration) –±—É–¥–µ—Ç –æ–ø–∏—Å–∞–Ω —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º.  \n",
    "–§—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—ë—Ç –∫–æ—Ä—Ç–µ–∂ (policy, V), –≥–¥–µ:\n",
    "\n",
    "- **policy** ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤ –≤–∏–¥–µ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è)\n",
    "- **V** ‚Äî —Ñ—É–Ω–∫—Ü–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    # –ù–∞—á–∏–Ω–∞–µ–º —Å–æ —Å–ª—É—á–∞–π–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:\n",
    "    # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π √ó –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π / —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º\n",
    "    policy = np.ones([environment.observation_space.n, environment.action_space.n]) / environment.action_space.n\n",
    "    # –°—á—ë—Ç—á–∏–∫ –æ—Ü–µ–Ω—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π\n",
    "    evaluated_policies = 1\n",
    "    # –ü–æ–≤—Ç–æ—Ä—è—Ç—å –¥–æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏–ª–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–µ–¥–µ–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π\n",
    "    for i in range(int(max_iterations)):\n",
    "        stable_policy = True\n",
    "        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é\n",
    "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é –∏ –ø—ã—Ç–∞–µ–º—Å—è —É–ª—É—á—à–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (—É–ª—É—á—à–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏)\n",
    "        for state in range(environment.observation_space.n):\n",
    "            # –¢–µ–∫—É—â–µ–µ –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤ —ç—Ç–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø–æ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏\n",
    "            current_action = np.argmax(policy[state])\n",
    "            # –°–º–æ—Ç—Ä–∏–º –Ω–∞ —à–∞–≥ –≤–ø–µ—Ä—ë–¥ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –ª–∏ —Ç–µ–∫—É—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ\n",
    "            # –ü—Ä–æ–≤–µ—Ä–∏–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ —ç—Ç–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # –í—ã–±–∏—Ä–∞–µ–º –±–æ–ª–µ–µ –≤—ã–≥–æ–¥–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ\n",
    "            best_action = np.argmax(action_value)\n",
    "            # –ï—Å–ª–∏ –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å\n",
    "            if current_action != best_action:\n",
    "                stable_policy = False\n",
    "                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∂–∞–¥–Ω–æ (greedy)\n",
    "                policy[state] = np.eye(environment.action_space.n)[best_action]\n",
    "        evaluated_policies += 1\n",
    "        # –ï—Å–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º —Å–æ—à—ë–ª—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "        if stable_policy:\n",
    "            print(f'–û—Ü–µ–Ω–µ–Ω–æ {evaluated_policies} —Å—Ç—Ä–∞—Ç–µ–≥–∏–π.')\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy Iteration](https://miro.medium.com/max/2400/1*2Gi8h7WzP4-vfyMzZnLuqw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration ‚Äî –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ $ \\pi^* $\n",
    "\n",
    "–ê–ª–≥–æ—Ä–∏—Ç–º –Ω–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π $ V(s) $.\n",
    "\n",
    "### üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "- $ \\theta > 0 $: –º–∞–ª–µ–Ω—å–∫–∏–π –ø–æ—Ä–æ–≥, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–π —Ç–æ—á–Ω–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "- $ \\gamma $: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "### üîÅ –®–∞–≥–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞:\n",
    "\n",
    "1. **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**:  \n",
    "–ó–∞–¥–∞—ë–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è $ V(s) $ –¥–ª—è –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π $ s \\in \\mathcal{S}^+ $, –∫—Ä–æ–º–µ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã—Ö:  \n",
    "$$\n",
    "V(\\text{terminal}) = 0\n",
    "$$\n",
    "\n",
    "2. **–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª**:  \n",
    "–ü–æ–≤—Ç–æ—Ä—è–µ–º –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—Ç—Å—è:\n",
    "\n",
    "```\n",
    "Œî ‚Üê 0  \n",
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è s ‚àà S:  \n",
    "‚ÄÉv ‚Üê V(s)  \n",
    "‚ÄÉV(s) ‚Üê max‚Çê ‚àë_{s', r} p(s', r | s, a) ¬∑ [r + Œ≥¬∑V(s')]  \n",
    "‚ÄÉŒî ‚Üê max(Œî, |v ‚àí V(s)|)  \n",
    "–ü–æ–≤—Ç–æ—Ä—è—Ç—å, –ø–æ–∫–∞ Œî ‚â• Œ∏  \n",
    "```\n",
    "\n",
    "- –ó–¥–µ—Å—å $ p(s', r \\mid s, a) $ ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ø–∞—Å—Ç—å –≤ $ s' $ —Å –Ω–∞–≥—Ä–∞–¥–æ–π $ r $, –≤—ã–±—Ä–∞–≤ –¥–µ–π—Å—Ç–≤–∏–µ $ a $ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è $ s $\n",
    "- –ú—ã –∏—â–µ–º –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ $ a $, –∫–æ—Ç–æ—Ä–æ–µ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–∂–∏–¥–∞–µ–º—É—é —Å—É–º–º—É –Ω–∞–≥—Ä–∞–¥\n",
    "\n",
    "3. **–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏** (–ø–æ—Å–ª–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏):  \n",
    "–ö–æ–≥–¥–∞ $ V(s) $ –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç –º–µ–Ω—è—Ç—å—Å—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a \\sum_{s', r} p(s', r | s, a) \\cdot [r + \\gamma V(s')]\n",
    "$$\n",
    "\n",
    "- –í –∫–∞–∂–¥–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ $ s $ –≤—ã–±–∏—Ä–∞–µ–º —Ç–∞–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ $ a $, –∫–æ—Ç–æ—Ä–æ–µ –≤–µ–¥—ë—Ç –∫ –Ω–∞–∏–±–æ–ª—å—à–µ–π –æ–∂–∏–¥–∞–µ–º–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "### üß† –°—É—Ç—å:\n",
    "- –ê–ª–≥–æ—Ä–∏—Ç–º –Ω–∞–ø—Ä—è–º—É—é —É–ª—É—á—à–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è $ V(s) $\n",
    "- –°—Ç—Ä–∞—Ç–µ–≥–∏—è $ \\pi(s) $ —Å—Ç—Ä–æ–∏—Ç—Å—è –≤ —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö $ V(s) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è value iteration.  \n",
    "–ê–ª–≥–æ—Ä–∏—Ç–º value iteration –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—Ö–æ–∂–∏–º –æ–±—Ä–∞–∑–æ–º –≤ –≤–∏–¥–µ –∫–æ–¥–∞:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –Ω—É–ª—è–º–∏ –¥–ª—è –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å—Ä–µ–¥—ã\n",
    "    V = np.zeros(environment.observation_space.n)\n",
    "    for i in range(int(max_iterations)):\n",
    "        # –£—Å–ª–æ–≤–∏–µ –¥–æ—Å—Ä–æ—á–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
    "        delta = 0\n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ\n",
    "        for state in range(environment.observation_space.n):\n",
    "            # –û–¥–∏–Ω —à–∞–≥ –≤–ø–µ—Ä—ë–¥ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –¥–µ–π—Å—Ç–≤–∏–π –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∏–±–æ–ª—å—à–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ Q(s,a)\n",
    "            best_action_value = np.max(action_value)\n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "            # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "            V[state] = best_action_value\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞, –º–æ–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è\n",
    "        if delta < theta:\n",
    "            print(f'–ò—Ç–µ—Ä–∞—Ü–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—à–ª–∞—Å—å –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ #{i}.')\n",
    "            break\n",
    "\n",
    "    # –°—Ç—Ä–æ–∏–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "    policy = np.zeros([environment.observation_space.n, environment.action_space.n])\n",
    "    for state in range(environment.observation_space.n):\n",
    "        # –û–¥–∏–Ω —à–∞–≥ –≤–ø–µ—Ä—ë–¥, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –ø–æ –Ω–∞–∏–±–æ–ª—å—à–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "        best_action = np.argmax(action_value)\n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: –≤ —ç—Ç–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ\n",
    "        policy[state, best_action] = 1.0\n",
    "    return policy, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–∫–æ–Ω–µ—Ü, –¥–∞–≤–∞–π—Ç–µ —Å—Ä–∞–≤–Ω–∏–º –æ–±–∞ –º–µ—Ç–æ–¥–∞, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–æ–π –∏–∑ –Ω–∏—Ö —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –æ–±—É—á–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è —Å—Ä–µ–¥—ã FrozenLake, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±–∞ –æ–ø–∏—Å–∞–Ω–Ω—ã—Ö –≤—ã—à–µ –ø–æ–¥—Ö–æ–¥–∞. –ó–∞—Ç–µ–º –º—ã –ø—Ä–æ–≤–µ—Ä–∏–º, –∫–∞–∫–∞—è –∏–∑ —Ç–µ—Ö–Ω–∏–∫ –ø–æ–∫–∞–∑–∞–ª–∞ –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —Å—Ä–∞–≤–Ω–∏–≤ —Å—Ä–µ–¥–Ω—é—é –Ω–∞–≥—Ä–∞–¥—É –ø–æ—Å–ª–µ 10 000 —ç–ø–∏–∑–æ–¥–æ–≤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for episode in range(n_episodes):\n",
    "        terminated = False\n",
    "        state = environment.reset()[0]\n",
    "        while not terminated:\n",
    "            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏\n",
    "            action = np.argmax(policy[state])\n",
    "            # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –∏ –Ω–∞–±–ª—é–¥–∞–µ–º —Ä–µ–∞–∫—Ü–∏—é —Å—Ä–µ–¥—ã\n",
    "            next_state, reward, terminated, truncated, info = environment.step(action)\n",
    "            # –°—É–º–º–∏—Ä—É–µ–º –æ–±—â–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ\n",
    "            total_reward += reward\n",
    "            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ\n",
    "            state = next_state\n",
    "            # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "    average_reward = total_reward / n_episodes\n",
    "    return wins, total_reward, average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ü–µ–Ω–µ–Ω–∞ –∑–∞ 66 –∏—Ç–µ—Ä–∞—Ü–∏–π.\n",
      "–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ü–µ–Ω–µ–Ω–∞ –∑–∞ 170 –∏—Ç–µ—Ä–∞—Ü–∏–π.\n",
      "–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ü–µ–Ω–µ–Ω–∞ –∑–∞ 428 –∏—Ç–µ—Ä–∞—Ü–∏–π.\n",
      "–û—Ü–µ–Ω–µ–Ω–æ 4 —Å—Ç—Ä–∞—Ç–µ–≥–∏–π.\n",
      "Policy Iteration :: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–±–µ–¥ –∑–∞ 10000 —ç–ø–∏–∑–æ–¥–æ–≤ = 8290\n",
      "Policy Iteration :: —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ 10000 —ç–ø–∏–∑–æ–¥–æ–≤ = 0.829\n",
      "–≤—Ä–µ–º—è = 0.06 —Å–µ–∫ \n",
      "\n",
      "\n",
      "–ò—Ç–µ—Ä–∞—Ü–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—à–ª–∞—Å—å –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ #523.\n",
      "Value Iteration :: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–±–µ–¥ –∑–∞ 10000 —ç–ø–∏–∑–æ–¥–æ–≤ = 8298\n",
      "Value Iteration :: —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ 10000 —ç–ø–∏–∑–æ–¥–æ–≤ = 0.8298\n",
      "–≤—Ä–µ–º—è = 0.07 —Å–µ–∫ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –ø—Ä–æ–≥–æ–Ω–∞\n",
    "n_episodes = 10000\n",
    "# –°–ø–∏—Å–æ–∫ —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏\n",
    "solvers = [('Policy Iteration', policy_iteration),\n",
    "           ('Value Iteration', value_iteration)]\n",
    "\n",
    "for iteration_name, iteration_func in solvers:\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ä–µ–¥—É Frozen Lake\n",
    "    environment = gym.make('FrozenLake-v1')\n",
    "    # –ò—â–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å –ø–æ–º–æ—â—å—é –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞\n",
    "    start = time.time()\n",
    "    policy, V = iteration_func(environment.unwrapped)\n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥–µ\n",
    "    wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "    print(f'{iteration_name} :: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–±–µ–¥ –∑–∞ {n_episodes} —ç–ø–∏–∑–æ–¥–æ–≤ = {wins}')\n",
    "    print(f'{iteration_name} :: —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ {n_episodes} —ç–ø–∏–∑–æ–¥–æ–≤ = {average_reward}')\n",
    "    print(f'–≤—Ä–µ–º—è = {elapsed:.2f} —Å–µ–∫ \\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
