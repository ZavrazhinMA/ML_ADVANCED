{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'FrozenLake-v0' gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy Iteration](https://miro.medium.com/max/2400/1*07AboseYfdjqknCq0kG8Ow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Policy Iteration (с использованием итеративной оценки) для приближённого вычисления $\\pi \\approx \\pi_*$\n",
    "\n",
    "#### 1. Инициализация\n",
    "\n",
    "Задать $V(s) \\in \\mathbb{R}$ и $\\pi(s) \\in \\mathcal{A}(s)$ произвольно для всех $s \\in \\mathcal{S}$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Оценка стратегии (Policy Evaluation)\n",
    "\n",
    "Цикл:\n",
    "\n",
    "* $\\Delta \\leftarrow 0$\n",
    "* Цикл по каждому $s \\in \\mathcal{S}$:\n",
    "\n",
    "  * $v \\leftarrow V(s)$\n",
    "  * $V(s) \\leftarrow \\sum_{s', r} p(s', r \\mid s, \\pi(s)) \\left[ r + \\gamma V(s') \\right]$\n",
    "  * $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n",
    "\n",
    "Пока $\\Delta \\geq \\theta$\n",
    "(где $\\theta$ — небольшое положительное число, задающее точность оценки)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Улучшение стратегии (Policy Improvement)\n",
    "\n",
    "* $\\text{policy-stable} \\leftarrow \\text{true}$\n",
    "* Для каждого $s \\in \\mathcal{S}$:\n",
    "\n",
    "  * $\\text{old-action} \\leftarrow \\pi(s)$\n",
    "  * $\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s',r} p(s', r \\mid s,a)\\left[ r + \\gamma V(s') \\right]$\n",
    "  * Если $\\text{old-action} \\neq \\pi(s)$, то $\\text{policy-stable} \\leftarrow \\text{false}$\n",
    "\n",
    "Если стратегия стабильна — остановиться и вернуть $V \\approx v_*$, $\\pi \\approx \\pi_*$;\n",
    "иначе перейти к шагу 2.\n",
    "\n",
    "---\n",
    "\n",
    "Если хочешь — могу переписать это как Python-функцию, понятную для Jupyter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание параметров для функции policy iteration\n",
    "\n",
    "**policy**: Двумерный массив размера n(S) × n(A), каждая ячейка представляет собой вероятность выбора действия a в состоянии s.\n",
    "\n",
    "**environment**: Инициализированный объект среды OpenAI Gym\n",
    "\n",
    "**discount_factor**: Коэффициент дисконтирования для MDP\n",
    "\n",
    "**theta**: Порог изменения функции ценности. Когда обновление значения становится меньше этого числа, итерации прекращаются.\n",
    "\n",
    "**max_iterations**: Максимальное количество итераций, чтобы избежать бесконечного выполнения программы\n",
    "\n",
    "Функция вернёт вектор размера nS, представляющий функцию ценности для каждого состояния.\n",
    "\n",
    "---\n",
    "\n",
    "Теперь начнём с шага оценки стратегии (policy evaluation). Цель — сходиться к истинной функции ценности для заданной стратегии π. Мы определим функцию, которая будет возвращать нужную функцию ценности.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Количество итераций оценки\n",
    "    evaluation_iterations = 1\n",
    "    # Инициализируем функцию ценности для каждого состояния нулём\n",
    "    V = np.zeros(environment.observation_space.n)\n",
    "    # Повторяем, пока изменение значений меньше порога\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Инициализируем изменение функции ценности нулём\n",
    "        delta = 0\n",
    "        # Проходим по каждому состоянию\n",
    "        for state in range(environment.observation_space.n):\n",
    "           # Начальное новое значение для текущего состояния\n",
    "           v = 0\n",
    "           # Перебираем все возможные действия из этого состояния\n",
    "           for action, action_probability in enumerate(policy[state]):\n",
    "             # Проверяем, каким будет следующее состояние\n",
    "             for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                  # Вычисляем ожидаемое значение\n",
    "                  v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
    "\n",
    "           # Вычисляем абсолютное изменение функции ценности\n",
    "           delta = max(delta, np.abs(V[state] - v))\n",
    "           # Обновляем значение функции ценности\n",
    "           V[state] = v\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        # Завершаем, если изменение функции ценности незначительно\n",
    "        if delta < theta:\n",
    "            print(f'Стратегия оценена за {evaluation_iterations} итераций.')\n",
    "            return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь переходим к части улучшения стратегии в алгоритме итерации стратегии (policy iteration).  \n",
    "Нам понадобится вспомогательная функция, которая будет делать «один шаг вперёд» (one-step lookahead), чтобы вычислить функцию ценности состояния.  \n",
    "Эта функция будет возвращать массив длины nA, содержащий ожидаемое значение для каждого действия.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "    action_values = np.zeros(environment.action_space.n)\n",
    "    for action in range(environment.action_space.n):\n",
    "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь общий процесс итерации стратегии (policy iteration) будет описан следующим образом.  \n",
    "Функция вернёт кортеж (policy, V), где:\n",
    "\n",
    "- **policy** — оптимальная стратегия в виде матрицы вероятностей (для каждого состояния и действия)\n",
    "- **V** — функция ценности для каждого состояния\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    # Начинаем со случайной стратегии:\n",
    "    # количество состояний × количество действий / равномерно по действиям\n",
    "    policy = np.ones([environment.observation_space.n, environment.action_space.n]) / environment.action_space.n\n",
    "    # Счётчик оценённых стратегий\n",
    "    evaluated_policies = 1\n",
    "    # Повторять до сходимости или достижения предела итераций\n",
    "    for i in range(int(max_iterations)):\n",
    "        stable_policy = True\n",
    "        # Оцениваем текущую стратегию\n",
    "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "        # Проходим по каждому состоянию и пытаемся улучшить выбранные действия (улучшение стратегии)\n",
    "        for state in range(environment.observation_space.n):\n",
    "            # Текущее лучшее действие в этом состоянии по текущей стратегии\n",
    "            current_action = np.argmax(policy[state])\n",
    "            # Смотрим на шаг вперёд и проверяем, оптимально ли текущее действие\n",
    "            # Проверим все возможные действия в этом состоянии\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Выбираем более выгодное действие\n",
    "            best_action = np.argmax(action_value)\n",
    "            # Если действие изменилось\n",
    "            if current_action != best_action:\n",
    "                stable_policy = False\n",
    "                # Обновляем стратегию жадно (greedy)\n",
    "                policy[state] = np.eye(environment.action_space.n)[best_action]\n",
    "        evaluated_policies += 1\n",
    "        # Если стратегия не изменилась — алгоритм сошёлся, возвращаем результат\n",
    "        if stable_policy:\n",
    "            print(f'Оценено {evaluated_policies} стратегий.')\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy Iteration](https://miro.medium.com/max/2400/1*2Gi8h7WzP4-vfyMzZnLuqw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration — для приближения оптимальной стратегии $ \\pi^* $\n",
    "\n",
    "Алгоритм находит оптимальную стратегию, итеративно обновляя ценности состояний $ V(s) $.\n",
    "\n",
    "### 🔧 Параметры:\n",
    "- $ \\theta > 0 $: маленький порог, определяющий точность сходимости\n",
    "- $ \\gamma $: коэффициент дисконтирования\n",
    "\n",
    "### 🔁 Шаги алгоритма:\n",
    "\n",
    "1. **Инициализация**:  \n",
    "Задаём начальные значения $ V(s) $ для всех состояний $ s \\in \\mathcal{S}^+ $, кроме терминальных:  \n",
    "$$\n",
    "V(\\text{terminal}) = 0\n",
    "$$\n",
    "\n",
    "2. **Основной цикл**:  \n",
    "Повторяем до тех пор, пока значения не стабилизируются:\n",
    "\n",
    "```\n",
    "Δ ← 0  \n",
    "Для каждого состояния s ∈ S:  \n",
    " v ← V(s)  \n",
    " V(s) ← maxₐ ∑_{s', r} p(s', r | s, a) · [r + γ·V(s')]  \n",
    " Δ ← max(Δ, |v − V(s)|)  \n",
    "Повторять, пока Δ ≥ θ  \n",
    "```\n",
    "\n",
    "- Здесь $ p(s', r \\mid s, a) $ — вероятность попасть в $ s' $ с наградой $ r $, выбрав действие $ a $ из состояния $ s $\n",
    "- Мы ищем лучшее действие $ a $, которое максимизирует ожидаемую сумму наград\n",
    "\n",
    "3. **Формирование стратегии** (после сходимости):  \n",
    "Когда $ V(s) $ перестаёт меняться существенно, определяем стратегию:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a \\sum_{s', r} p(s', r | s, a) \\cdot [r + \\gamma V(s')]\n",
    "$$\n",
    "\n",
    "- В каждом состоянии $ s $ выбираем такое действие $ a $, которое ведёт к наибольшей ожидаемой ценности.\n",
    "\n",
    "### 🧠 Суть:\n",
    "- Алгоритм напрямую улучшает значения $ V(s) $\n",
    "- Стратегия $ \\pi(s) $ строится в самом конце на основе финальных $ V(s) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры определяются аналогично для value iteration.  \n",
    "Алгоритм value iteration можно реализовать похожим образом в виде кода:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Инициализируем функцию ценности состояний нулями для всех состояний среды\n",
    "    V = np.zeros(environment.observation_space.n)\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Условие досрочной остановки\n",
    "        delta = 0\n",
    "        # Обновляем каждое состояние\n",
    "        for state in range(environment.observation_space.n):\n",
    "            # Один шаг вперёд для расчёта ценностей действий из состояния\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Выбираем лучшее действие на основе наибольшей ценности Q(s,a)\n",
    "            best_action_value = np.max(action_value)\n",
    "            # Вычисляем изменение ценности\n",
    "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "            # Обновляем функцию ценности для текущего состояния\n",
    "            V[state] = best_action_value\n",
    "        # Проверка, можно ли остановиться\n",
    "        if delta < theta:\n",
    "            print(f'Итерация ценности сошлась на итерации #{i}.')\n",
    "            break\n",
    "\n",
    "    # Строим детерминированную стратегию на основе оптимальной функции ценности\n",
    "    policy = np.zeros([environment.observation_space.n, environment.action_space.n])\n",
    "    for state in range(environment.observation_space.n):\n",
    "        # Один шаг вперёд, чтобы найти лучшее действие из текущего состояния\n",
    "        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "        # Выбираем лучшее действие по наибольшей ценности\n",
    "        best_action = np.argmax(action_value)\n",
    "        # Обновляем стратегию: в этом состоянии выбираем только лучшее действие\n",
    "        policy[state, best_action] = 1.0\n",
    "    return policy, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, давайте сравним оба метода, чтобы понять, какой из них работает лучше на практике. Для этого мы попытаемся обучить оптимальную стратегию для среды FrozenLake, используя оба описанных выше подхода. Затем мы проверим, какая из техник показала лучший результат, сравнив среднюю награду после 10 000 эпизодов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for episode in range(n_episodes):\n",
    "        terminated = False\n",
    "        state = environment.reset()[0]\n",
    "        while not terminated:\n",
    "            # Выбираем лучшее действие в текущем состоянии\n",
    "            action = np.argmax(policy[state])\n",
    "            # Выполняем действие и наблюдаем реакцию среды\n",
    "            next_state, reward, terminated, truncated, info = environment.step(action)\n",
    "            # Суммируем общее вознаграждение\n",
    "            total_reward += reward\n",
    "            # Переходим в новое состояние\n",
    "            state = next_state\n",
    "            # Считаем количество успешных эпизодов\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "    average_reward = total_reward / n_episodes\n",
    "    return wins, total_reward, average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия оценена за 66 итераций.\n",
      "Стратегия оценена за 170 итераций.\n",
      "Стратегия оценена за 428 итераций.\n",
      "Оценено 4 стратегий.\n",
      "Policy Iteration :: количество побед за 10000 эпизодов = 8290\n",
      "Policy Iteration :: средняя награда за 10000 эпизодов = 0.829\n",
      "время = 0.06 сек \n",
      "\n",
      "\n",
      "Итерация ценности сошлась на итерации #523.\n",
      "Value Iteration :: количество побед за 10000 эпизодов = 8298\n",
      "Value Iteration :: средняя награда за 10000 эпизодов = 0.8298\n",
      "время = 0.07 сек \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Количество эпизодов для прогона\n",
    "n_episodes = 10000\n",
    "# Список функций для поиска оптимальной стратегии\n",
    "solvers = [('Policy Iteration', policy_iteration),\n",
    "           ('Value Iteration', value_iteration)]\n",
    "\n",
    "for iteration_name, iteration_func in solvers:\n",
    "    # Загружаем среду Frozen Lake\n",
    "    environment = gym.make('FrozenLake-v1')\n",
    "    # Ищем оптимальную стратегию с помощью выбранного метода\n",
    "    start = time.time()\n",
    "    policy, V = iteration_func(environment.unwrapped)\n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    # Применяем найденную стратегию в реальной среде\n",
    "    wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "    print(f'{iteration_name} :: количество побед за {n_episodes} эпизодов = {wins}')\n",
    "    print(f'{iteration_name} :: средняя награда за {n_episodes} эпизодов = {average_reward}')\n",
    "    print(f'время = {elapsed:.2f} сек \\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
